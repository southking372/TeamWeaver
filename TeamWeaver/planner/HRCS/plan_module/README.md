# HRCS Plan Moduleæ¶æ„è¯´æ˜

## ğŸ“‹ æ¦‚è¿°

`plan_module/`æ–‡ä»¶å¤¹åŒ…å«äº†MIQP-LLMæ··åˆè§„åˆ’ç³»ç»Ÿçš„æ ¸å¿ƒæ‰§è¡Œæ¨¡å—ï¼Œè´Ÿè´£å°†MIQPä¼˜åŒ–ç»“æœä¸LLMæ¨ç†èƒ½åŠ›ç›¸ç»“åˆï¼Œå®ç°æ™ºèƒ½ã€ç¨³å®šçš„ä»»åŠ¡è§„åˆ’å’Œæ‰§è¡Œç®¡ç†ã€‚è¿™äº›æ¨¡å—é‡‡ç”¨äº†**ç›®æ ‡å¯¼å‘è®¾è®¡**ï¼Œä¸“æ³¨äºå®ç°é«˜è´¨é‡çš„è§„åˆ’å†³ç­–å’Œæ‰§è¡Œæ§åˆ¶ã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡ç†å¿µ

### æ ¸å¿ƒè®¾è®¡åŸåˆ™
- **ç›®æ ‡å¯¼å‘**ï¼šæä¾›ç›®æ ‡æŒ‡å¯¼è€Œéç›´æ¥åŠ¨ä½œå‘½ä»¤ï¼Œä¿æŒLLMçš„å†³ç­–è‡ªä¸»æ€§
- **æ™ºèƒ½å¢å¼º**ï¼šMIQPä¼˜åŒ–ä¸ºLLMæä¾›å…¨å±€æœ€ä¼˜çš„å®è§‚æŒ‡å¯¼
- **åé¦ˆé©±åŠ¨**ï¼šåŸºäºæ‰§è¡Œåé¦ˆè¿›è¡Œå­¦ä¹ å’Œè°ƒæ•´ï¼Œæå‡ç³»ç»Ÿç¨³å®šæ€§
- **æ¨¡å—è§£è€¦**ï¼šæ¯ä¸ªæ¨¡å—èŒè´£æ˜ç¡®ï¼Œä¾¿äºç‹¬ç«‹æµ‹è¯•å’Œæ‰©å±•

### æ‰§è¡Œæµç¨‹
```
MIQPä¼˜åŒ– â†’ TaskHelperåˆ†é… â†’ ActionManageréªŒè¯ â†’ 
PromptBuilderå¢å¼º â†’ ErrorHandleræ¢å¤ â†’ ExecutionManageræ›´æ–° â†’ 
FeedbackManagerè¯„ä¼°
```

## ğŸ“¦ æ¨¡å—è¯¦è§£

### 1. `action_manager.py` - æ™ºèƒ½åŠ¨ä½œç®¡ç†å™¨
**æ ¸å¿ƒèŒè´£**ï¼šè§£æLLMå“åº”å¹¶éªŒè¯åŠ¨ä½œçš„ç›®æ ‡ä¸€è‡´æ€§

#### ä¸»è¦åŠŸèƒ½ï¼š
- **åŠ¨ä½œè§£æ**ï¼šå°†LLMå“åº”è½¬æ¢ä¸ºç»“æ„åŒ–çš„é«˜çº§åŠ¨ä½œ
- **ç›®æ ‡å¯¼å‘éªŒè¯**ï¼šéªŒè¯LLMåŠ¨ä½œæ˜¯å¦æœ‰åŠ©äºå®ç°é˜¶æ®µç›®æ ‡
- **æ™ºèƒ½è°ƒæ•´**ï¼šåœ¨ä¿æŒLLMå†³ç­–è‡ªä¸»æ€§çš„å‰æä¸‹æä¾›ç›®æ ‡å¯¼å‘å»ºè®®

#### å…³é”®æ–¹æ³•ï¼š
```python
# è§£æLLMå“åº”ä¸­çš„é«˜çº§åŠ¨ä½œ
def parse_high_level_actions(llm_response: str, agents) -> Dict[int, Tuple[str, str, Optional[str]]]

# ç›®æ ‡å¯¼å‘çš„åŠ¨ä½œéªŒè¯å’Œè°ƒæ•´
def adjust_actions_with_phase_awareness(
    high_level_actions, agent_task_assignments, current_phase, agents
) -> Dict[int, Tuple[str, str, Optional[str]]]

# éªŒè¯åŠ¨ä½œæ˜¯å¦æ¨è¿›ç›®æ ‡å®ç°
def _action_advances_objective(action_name, action_target, objective) -> bool

# ä¸ºç›®æ ‡å»ºè®®åˆé€‚çš„åŠ¨ä½œ
def _suggest_action_for_objective(objective, original_action) -> Tuple[str, str, Optional[str]]
```

#### è®¾è®¡ç‰¹ç‚¹ï¼š
```python
# ç›®æ ‡å¯¼å‘éªŒè¯é€»è¾‘ç¤ºä¾‹
if self._action_advances_objective(llm_action, llm_target, primary_objective):
    # LLMçš„åŠ¨ä½œæœ‰åŠ©äºç›®æ ‡å®ç°ï¼Œä¿æŒä¸å˜
    adjusted_actions[agent_id] = action_tuple
    print(f"LLM action '{llm_action}[{llm_target}]' advances objective âœ“")
else:
    # æä¾›ç›®æ ‡å¯¼å‘çš„å»ºè®®ä½†ä¿æŒä¸€å®šçµæ´»æ€§
    if self._is_exploration_reasonable(llm_action, primary_objective):
        adjusted_actions[agent_id] = action_tuple
```

### 2. `task_helper.py` - åŸºäº"ç®€å†"çš„æ™ºèƒ½ä»»åŠ¡åˆ†é…ç³»ç»Ÿ
**æ ¸å¿ƒèŒè´£**ï¼šåŸºäºAgentèƒ½åŠ›ç‰¹å¾è¿›è¡Œæ™ºèƒ½ä»»åŠ¡åˆ†é…

#### ä¸»è¦åŠŸèƒ½ï¼š
- **Agentç®€å†ç®¡ç†**ï¼šç»´æŠ¤æ¯ä¸ªAgentçš„èƒ½åŠ›ã€æ€§èƒ½ç‰¹è´¨å’ŒåŠ¨æ€çŠ¶æ€
- **æ™ºèƒ½è¯„åˆ†åˆ†é…**ï¼šåŸºäºç¡¬çº¦æŸ+è½¯çº¦æŸçš„åŒå±‚åŒ¹é…æœºåˆ¶
- **åŠ¨æ€å­¦ä¹ **ï¼šæ ¹æ®æ‰§è¡Œåé¦ˆæ›´æ–°Agentæ€§èƒ½è¯„ä¼°
- **è´Ÿè½½å‡è¡¡**ï¼šé¿å…å•ä¸ªAgentè¿‡è½½

#### Agentç®€å†ç»“æ„ï¼š
```python
@dataclass
class AgentResume:
    agent_id: int
    name: str
    
    # ç¡¬çº¦æŸï¼šAgentèƒ½æ‰§è¡Œçš„ä»»åŠ¡ç±»å‹
    capabilities: List[str] = field(default_factory=list)
    
    # è½¯çº¦æŸï¼šæ€§èƒ½ç‰¹è´¨è¯„åˆ† (0-1)
    performance_traits: Dict[str, float] = field(default_factory=dict)
    
    # åŠ¨æ€çŠ¶æ€ï¼šå®æ—¶å†³ç­–ä¿¡æ¯
    current_task_load: int = 0
    position: Optional[np.ndarray] = None
    history: List[Dict[str, Any]] = field(default_factory=list)
```

#### å…³é”®æ–¹æ³•ï¼š
```python
# æ ¸å¿ƒè¯„åˆ†å‡½æ•°ï¼šè®¡ç®—Agentå¯¹ä»»åŠ¡çš„é€‚åº”æ€§åˆ†æ•°
def _calculate_assignment_score(task: Dict[str, Any], resume: AgentResume) -> float

# åŸºäºè¯„åˆ†çš„æ™ºèƒ½ä»»åŠ¡åˆ†é…
def _distribute_tasks_with_scoring(tasks, agent_ids) -> Dict[int, List[Dict[str, Any]]]

# æ ¹æ®åé¦ˆæ›´æ–°æ€§èƒ½ç‰¹è´¨
def _update_traits_from_feedback(resume: AgentResume, feedback: str)

# æ›´æ–°Agentç®€å†ä¸Šä¸‹æ–‡
def update_resumes_from_context(world_state, agent_feedback)
```

#### æ™ºèƒ½åˆ†é…ç¤ºä¾‹ï¼š
```python
# Agent 0: "Workhorse" - åŸºç¡€èƒ½åŠ›å¼ºï¼Œé€Ÿåº¦å¿«
Agent_0_Resume = {
    'capabilities': ['Navigate', 'Explore', 'Pick', 'Place', 'Rearrange'],
    'performance_traits': {
        'speed': 0.9,           # é«˜é€Ÿåº¦
        'precision': 0.6,       # ä¸­ç­‰ç²¾åº¦
        'exploration_bias': 0.5 # ä¸­ç­‰æ¢ç´¢å€¾å‘
    }
}

# Agent 1: "Specialist" - ç²¾å¯†æ“ä½œï¼Œç‰¹æ®ŠæŠ€èƒ½
Agent_1_Resume = {
    'capabilities': ['Navigate', 'Pick', 'Place', 'Clean', 'Fill', 'PowerOn'],
    'performance_traits': {
        'speed': 0.6,                    # ä¸­ç­‰é€Ÿåº¦
        'precision': 0.9,                # é«˜ç²¾åº¦
        'liquid_handling_skill': 0.95,   # æ¶²ä½“å¤„ç†ä¸“é•¿
        'power_control_skill': 0.9       # ç”µæºæ§åˆ¶ä¸“é•¿
    }
}
```

### 3. `miqp_solver_wrapper.py` - MIQPæ±‚è§£å™¨åŒ…è£…å™¨
**æ ¸å¿ƒèŒè´£**ï¼šç®¡ç†MIQPä¼˜åŒ–å‚æ•°å’Œæ±‚è§£è¿‡ç¨‹

#### ä¸»è¦åŠŸèƒ½ï¼š
- **å‚æ•°ç®¡ç†**ï¼šåˆå§‹åŒ–å’Œç»´æŠ¤ScenarioConfigTaskå’ŒOptimizationConfigTask
- **é˜¶æ®µæ„ŸçŸ¥æ±‚è§£**ï¼šæ”¯æŒé’ˆå¯¹å½“å‰é˜¶æ®µçš„åŠ¨æ€ä»»åŠ¡å®ä¾‹æ±‚è§£
- **RTAé›†æˆ**ï¼šå°è£…RTAæ±‚è§£å™¨çš„è°ƒç”¨æ¥å£

#### å…³é”®æ–¹æ³•ï¼š
```python
# è®¾ç½®MIQPå‚æ•°
def task_plan_MIQP_set(agents: List["Agent"])

# é˜¶æ®µæ„ŸçŸ¥çš„MIQPæ±‚è§£
def task_plan_MIQP_solve_phase_aware(x, t, phase_task_info, agents)

# é‡ç½®æ±‚è§£å™¨çŠ¶æ€
def reset()
```

#### æ±‚è§£æµç¨‹ï¼š
```python
# å…¸å‹æ±‚è§£è°ƒç”¨
alpha, u, delta, time_to_solve, opt_sol_info = self.miqp_solver_wrapper.task_plan_MIQP_solve_phase_aware(
    x,                  # å½“å‰çŠ¶æ€
    t,                  # æ—¶é—´
    phase_task_info,    # é˜¶æ®µä»»åŠ¡ä¿¡æ¯
    self._agents        # Agentåˆ—è¡¨
)
```

### 4. `prompt_builder.py` - æ™ºèƒ½æç¤ºæ„å»ºå™¨
**æ ¸å¿ƒèŒè´£**ï¼šæ„å»ºMIQPæŒ‡å¯¼çš„ç›®æ ‡å¯¼å‘æç¤º

#### ä¸»è¦åŠŸèƒ½ï¼š
- **MIQPæŒ‡å¯¼æ„å»º**ï¼šå°†ä¼˜åŒ–ç»“æœè½¬æ¢ä¸ºLLMå¯ç†è§£çš„ç›®æ ‡æè¿°
- **ç›®æ ‡å¯¼å‘æ ¼å¼åŒ–**ï¼šæè¿°è¦å®ç°çš„ç›®æ ‡è€Œéå…·ä½“æ‰§è¡Œæ­¥éª¤
- **ä¸Šä¸‹æ–‡å¢å¼º**ï¼šæ•´åˆä¸–ç•ŒçŠ¶æ€å’Œé˜¶æ®µä¿¡æ¯

#### å…³é”®æ–¹æ³•ï¼š
```python
# æ„å»ºMIQPæŒ‡å¯¼ä¿¡æ¯
def build_miqp_guidance_addition(
    current_phase_tasks, agent_task_assignments, current_phase, 
    perception_connector, alpha_result=None, world_state=None
) -> str

# æ„å»ºé˜¶æ®µæ„ŸçŸ¥çš„å¢å¼ºæç¤ºï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨å¢é‡æ–¹å¼ï¼‰
def build_phase_aware_prompt_with_miqp_guidance(...)
```

#### ç›®æ ‡å¯¼å‘è®¾è®¡ç¤ºä¾‹ï¼š
```python
# ä¼ ç»ŸåŠ¨ä½œæŒ‡ä»¤ï¼ˆé¿å…ï¼‰
"Execute Pick[apple]"

# ç›®æ ‡å¯¼å‘æŒ‡å¯¼ï¼ˆæ¨èï¼‰
"achieve pickup of apple (requires navigation first if not nearby)"
```

### 5. `error_handler.py` - æ™ºèƒ½é”™è¯¯å¤„ç†å™¨
**æ ¸å¿ƒèŒè´£**ï¼šæ£€æµ‹æ‰§è¡Œå¤±è´¥å¹¶æä¾›æ™ºèƒ½æ¢å¤ç­–ç•¥

#### ä¸»è¦åŠŸèƒ½ï¼š
- **å¤±è´¥åˆ†æ**ï¼šåˆ†æAgentçŠ¶æ€åé¦ˆï¼Œè¯†åˆ«å¤±è´¥æ¨¡å¼
- **æ¢å¤ç­–ç•¥**ï¼šåŸºäºå¤±è´¥ç±»å‹ç”Ÿæˆé’ˆå¯¹æ€§çš„æ¢å¤åŠ¨ä½œ
- **å†å²è·Ÿè¸ª**ï¼šç»´æŠ¤é”™è¯¯å†å²å’Œæ¢å¤å°è¯•è®°å½•

#### å…³é”®æ–¹æ³•ï¼š
```python
# åº”ç”¨æ™ºèƒ½é”™è¯¯æ¢å¤
def recover_and_log_assignments(
    agent_task_assignments, last_high_level_actions, latest_agent_response
) -> Dict[int, List[Dict[str, Any]]]

# åˆ†æå¤±è´¥å¹¶å»ºè®®æ¢å¤åŠ¨ä½œ
def analyze_failure_and_suggest_recovery(
    agent_id, status, current_action
) -> Optional[Tuple[str, str, Optional[str]]]
```

#### æ™ºèƒ½æ¢å¤é€»è¾‘ï¼š
```python
# æ¢å¤ç­–ç•¥ç¤ºä¾‹
if action_name == "Pick" and "not close enough" in status_lower:
    return ("Navigate", target, target)  # å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®
elif action_name == "Pick" and "object not found" in status_lower:
    return ("Explore", "environment", "environment")  # æ¢ç´¢å¯»æ‰¾å¯¹è±¡
elif "collision" in status_lower:
    return ("Wait", "", "")  # ç­‰å¾…é¿å…å†²çª
```

### 6. `execution_manager.py` - æ‰§è¡Œç®¡ç†å™¨
**æ ¸å¿ƒèŒè´£**ï¼šç®¡ç†è§„åˆ’å‘¨æœŸçš„æœ€ç»ˆæ‰§è¡Œæ­¥éª¤

#### ä¸»è¦åŠŸèƒ½ï¼š
- **åœºæ™¯å‚æ•°æ›´æ–°**ï¼šåŸºäºé€‰å®šçš„é«˜çº§åŠ¨ä½œæ›´æ–°åœºæ™¯é…ç½®
- **ActionUpdateré›†æˆ**ï¼šè°ƒç”¨ActionUpdaterå¤„ç†åŠ¨ä½œè½¬æ¢
- **æ‰§è¡ŒçŠ¶æ€è·Ÿè¸ª**ï¼šç»´æŠ¤æ‰§è¡Œç›¸å…³çš„çŠ¶æ€ä¿¡æ¯

#### å…³é”®æ–¹æ³•ï¼š
```python
# ä¸ºæ‰§è¡Œæ›´æ–°åœºæ™¯é…ç½®
def update_scenario_for_execution(
    scenario_config, world_state, high_level_actions
) -> None
```

#### è°ƒç”¨ç¤ºä¾‹ï¼š
```python
# åœ¨llm_planner_miqp.pyä¸­çš„ä½¿ç”¨
self.execution_manager.update_scenario_for_execution(
    self.miqp_solver_wrapper.scenario_params,
    world_state,
    adjusted_actions
)
```

### 7. `feedback_manager.py` - åé¦ˆç®¡ç†å™¨
**æ ¸å¿ƒèŒè´£**ï¼šç®¡ç†æ‰§è¡Œåé¦ˆå’Œé˜¶æ®µæ¨è¿›

#### ä¸»è¦åŠŸèƒ½ï¼š
- **çŠ¶æ€æå–**ï¼šä»å¤šç§æ¥æºè·å–Agentå®ŒæˆçŠ¶æ€
- **é˜¶æ®µè¯„ä¼°**ï¼šæ£€æŸ¥å½“å‰é˜¶æ®µæ˜¯å¦å®Œæˆ
- **é˜¶æ®µæ¨è¿›**ï¼šç®¡ç†é˜¶æ®µè½¬æ¢é€»è¾‘
- **æ€§èƒ½è·Ÿè¸ª**ï¼šç»´æŠ¤Agentæ€§èƒ½æŒ‡æ ‡

#### å…³é”®æ–¹æ³•ï¼š
```python
# è·å–Agentå®ŒæˆçŠ¶æ€
def get_agent_completion_statuses(agents, latest_agent_response) -> Dict[int, str]

# æ£€æŸ¥å¹¶æ¨è¿›é˜¶æ®µ
def check_and_advance_phase(
    perception_connector, agents, latest_agent_response, current_phase
) -> bool
```

#### æ™ºèƒ½çŠ¶æ€è·å–ï¼š
```python
# ä¼˜å…ˆçº§ç­–ç•¥ï¼šæœ€æ–°å“åº” > AgentçŠ¶æ€ > é»˜è®¤å€¼
def get_agent_completion_statuses(agents, latest_agent_response):
    if latest_agent_response:
        # ä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„å“åº”
        return latest_agent_response
    else:
        # å¤‡ç”¨ï¼šè·å–AgentçŠ¶æ€æè¿°
        return {agent.uid: agent.get_last_state_description() for agent in agents}
```

## ğŸ”„ æ¨¡å—é—´åä½œæµç¨‹

### åœ¨replanæ–¹æ³•ä¸­çš„è°ƒç”¨åºåˆ—

```python
# Step 8: æ™ºèƒ½ä»»åŠ¡åˆ†é…
agent_task_assignments = self.task_helper.assign_tasks_for_phase(
    current_phase_tasks, alpha, phase_task_info, self._agents, ...
)

# Step 9: æ™ºèƒ½é”™è¯¯æ¢å¤  
agent_task_assignments = self.error_handler.recover_and_log_assignments(
    agent_task_assignments, self.last_high_level_actions, self.latest_agent_response
)

# Step 10: æç¤ºæ„å»º
miqp_guidance = self.prompt_builder.build_miqp_guidance_addition(
    current_phase_tasks, agent_task_assignments, current_phase, ...
)

# Step 11: LLMè°ƒç”¨ï¼ˆåé¦ˆå¢å¼ºï¼‰
llm_response = self.llm.generate(self.curr_prompt + miqp_guidance, self.stopword)

# Step 12: åŠ¨ä½œè§£æå’ŒéªŒè¯
high_level_actions = self.action_manager.parse_high_level_actions(llm_response, self._agents)
adjusted_actions = self.action_manager.adjust_actions_with_phase_awareness(
    high_level_actions, agent_task_assignments, current_phase, self._agents
)

# Step 13: æ‰§è¡Œå‚æ•°æ›´æ–°
self.execution_manager.update_scenario_for_execution(
    self.miqp_solver_wrapper.scenario_params, world_state, adjusted_actions
)

# Step 15: åé¦ˆå’Œé˜¶æ®µç®¡ç†
self._phase_transition_pending = self.feedback_manager.check_and_advance_phase(
    self.perception_connector, self._agents, self.latest_agent_response, current_phase
)
```

### æ¨¡å—ä¾èµ–å…³ç³»å›¾
```
MIQPSolverWrapper â”€â”€â”€â”€â”€â”
                       â”œâ”€â”€â†’ TaskHelper â”€â”€â†’ ActionManager â”€â”€â†’ PromptBuilder
ErrorHandler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
                                                   â–¼
ExecutionManager â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM Response
    â”‚                                              â”‚
    â–¼                                              â–¼
FeedbackManager â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Agent Execution Results
```

## ğŸ“š ä½¿ç”¨æŒ‡å—

### åˆå§‹åŒ–
```python
from habitat_llm.planner.HRCS.plan_module.action_manager import ActionManager
from habitat_llm.planner.HRCS.plan_module.task_helper import TaskHelper
from habitat_llm.planner.HRCS.plan_module.miqp_solver_wrapper import MIQPSolverWrapper
# ... å…¶ä»–æ¨¡å—

# åœ¨LLMPlanner.__init__ä¸­åˆå§‹åŒ–
self.miqp_solver_wrapper = MIQPSolverWrapper()
self.task_helper = TaskHelper(self._agents)
self.action_manager = ActionManager(self.actions_parser)
self.error_handler = ErrorHandler()
self.prompt_builder = PromptBuilder(plan_config, env_interface)
self.execution_manager = ExecutionManager()
self.feedback_manager = FeedbackManager()
```

### å…¸å‹ä½¿ç”¨æµç¨‹
```python
# 1. è®¾ç½®MIQPå‚æ•°
self.miqp_solver_wrapper.task_plan_MIQP_set(self._agents)

# 2. æ™ºèƒ½ä»»åŠ¡åˆ†é…
assignments = self.task_helper.assign_tasks_for_phase(...)

# 3. é”™è¯¯æ¢å¤
recovered_assignments = self.error_handler.recover_and_log_assignments(...)

# 4. æ„å»ºå¢å¼ºæç¤º
guidance = self.prompt_builder.build_miqp_guidance_addition(...)

# 5. è§£æå’ŒéªŒè¯åŠ¨ä½œ
actions = self.action_manager.parse_high_level_actions(...)
adjusted_actions = self.action_manager.adjust_actions_with_phase_awareness(...)

# 6. æ›´æ–°æ‰§è¡Œå‚æ•°
self.execution_manager.update_scenario_for_execution(...)

# 7. ç®¡ç†åé¦ˆå’Œé˜¶æ®µ
phase_transition = self.feedback_manager.check_and_advance_phase(...)
```

## ğŸ¯ è®¾è®¡ç‰¹è‰²

### 1. ç›®æ ‡å¯¼å‘è€ŒéåŠ¨ä½œå¯¼å‘
- **ä¼ ç»Ÿæ–¹å¼**ï¼šç›´æ¥æŒ‡å®š"Execute Pick[object]"
- **æœ¬ç³»ç»Ÿ**ï¼šæè¿°ç›®æ ‡"achieve pickup of object (requires navigation first)"

### 2. æ™ºèƒ½Agentç®€å†ç³»ç»Ÿ
- **èƒ½åŠ›è¾¹ç•Œ**ï¼šç¡¬çº¦æŸç¡®ä¿ä»»åŠ¡å¯æ‰§è¡Œæ€§
- **æ€§èƒ½ä¼˜åŒ–**ï¼šè½¯çº¦æŸä¼˜åŒ–åˆ†é…æ•ˆç‡
- **åŠ¨æ€å­¦ä¹ **ï¼šæ ¹æ®åé¦ˆè°ƒæ•´Agentè¯„ä¼°

### 3. åé¦ˆå¢å¼ºçš„è§„åˆ’å¾ªç¯
- **å†å²ä¿æŒ**ï¼šç»´æŠ¤å®Œæ•´çš„å¯¹è¯å†å²å’Œæ‰§è¡Œåé¦ˆ
- **MIQPå¢å¼º**ï¼šä¼˜åŒ–æŒ‡å¯¼ä½œä¸ºå¢é‡ä¿¡æ¯è€Œéæ›¿æ¢
- **é˜¶æ®µæ„ŸçŸ¥**ï¼šåŸºäºæ‰§è¡Œé˜¶æ®µçš„åŠ¨æ€è§„åˆ’è°ƒæ•´

## ğŸ§ª æµ‹è¯•å»ºè®®

### å•å…ƒæµ‹è¯•é‡ç‚¹
- **ActionManager**ï¼šæµ‹è¯•ç›®æ ‡éªŒè¯é€»è¾‘å’ŒåŠ¨ä½œè°ƒæ•´ç­–ç•¥
- **TaskHelper**ï¼šéªŒè¯è¯„åˆ†ç®—æ³•å’Œåˆ†é…å…¬å¹³æ€§
- **ErrorHandler**ï¼šæµ‹è¯•å¤±è´¥è¯†åˆ«å’Œæ¢å¤ç­–ç•¥çš„å‡†ç¡®æ€§
- **PromptBuilder**ï¼šéªŒè¯æç¤ºæ ¼å¼å’Œç›®æ ‡æè¿°çš„æ¸…æ™°æ€§

### é›†æˆæµ‹è¯•é‡ç‚¹
- **MIQP-LLMä¸€è‡´æ€§**ï¼šéªŒè¯ä¼˜åŒ–å»ºè®®ä¸LLMå†³ç­–çš„åè°ƒæ€§
- **åé¦ˆé—­ç¯**ï¼šæµ‹è¯•æ‰§è¡Œåé¦ˆå¯¹åç»­è§„åˆ’çš„å½±å“
- **é˜¶æ®µæ¨è¿›**ï¼šéªŒè¯é˜¶æ®µè½¬æ¢çš„æ­£ç¡®æ€§å’ŒåŠæ—¶æ€§

## ğŸ”§ æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°çš„Agentç±»å‹
1. åœ¨`TaskHelper`ä¸­æ‰©å±•`_initialize_agent_resumes()`
2. æ·»åŠ æ–°çš„èƒ½åŠ›ç‰¹è´¨å’Œè¯„åˆ†é€»è¾‘
3. æ›´æ–°`_calculate_assignment_score()`æ–¹æ³•

### å¢åŠ æ–°çš„é”™è¯¯ç±»å‹
1. åœ¨`ErrorHandler`ä¸­æ‰©å±•`analyze_failure_and_suggest_recovery()`
2. æ·»åŠ æ–°çš„å¤±è´¥æ¨¡å¼è¯†åˆ«é€»è¾‘
3. è®¾è®¡ç›¸åº”çš„æ¢å¤ç­–ç•¥

### æ‰©å±•MIQPæ±‚è§£å™¨
1. åœ¨`MIQPSolverWrapper`ä¸­æ·»åŠ æ–°çš„æ±‚è§£æ¨¡å¼
2. æ‰©å±•phase_task_infoç»“æ„
3. æ›´æ–°æ±‚è§£å™¨è°ƒç”¨æ¥å£
